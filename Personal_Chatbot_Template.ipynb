{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde3b209",
   "metadata": {
    "id": "bde3b209"
   },
   "source": [
    "# Personal Chatbot Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585202f",
   "metadata": {
    "id": "e585202f"
   },
   "source": [
    "This template can be used to build a personal chatbot that searches the most relevant personal documents and uses these documents to provide a language model (in this case OpenAI's gpt-3.5) with relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d84cd",
   "metadata": {
    "id": "6c9d84cd"
   },
   "source": [
    "This template follows the steps of Langchain's [retrieval-augmented generation (RAG) guideline](https://python.langchain.com/docs/use_cases/question_answering.html). The steps are the following:\n",
    "\n",
    "1. Document Loading: Loading all the desired documents and sources into the right format via specific document loaders (loaders for each file type exist, e.g. txt or pdf).\n",
    "2. Splitting: Splitting the loaded documents into smaller manageable chuncks that fit into language models.\n",
    "3. Storage: Storing the splitted and embedded documents in a vectorstore (database).\n",
    "4. Retrieval: Retrieving the most relevant document splits based on a similarity measure.\n",
    "5. Output: Feeding the language model the relevant context and obtaining the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfddc2f",
   "metadata": {
    "id": "3dfddc2f"
   },
   "source": [
    "![Image of the Retrieval Augmented Generation](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da3cf9",
   "metadata": {
    "id": "b3da3cf9"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5405d",
   "metadata": {},
   "source": [
    "The following code snippet sets the paths to the files that should be included in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505eca15",
   "metadata": {
    "id": "505eca15"
   },
   "outputs": [],
   "source": [
    "#getting the paths to all relevant documents\n",
    "from os import listdir\n",
    "path_data = \"path to your data\"\n",
    "doc_paths = [path_data + x for x in listdir(path_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ada10",
   "metadata": {
    "id": "e26ada10"
   },
   "source": [
    "## 1. Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1d14a",
   "metadata": {
    "id": "5bf1d14a"
   },
   "source": [
    "To keep the process as simple as possible we just use one file type (word documents). If you want to use other file types you need additional loaders for those document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uLTcq91h58h7",
   "metadata": {
    "id": "uLTcq91h58h7"
   },
   "outputs": [],
   "source": [
    "#this code installs necessary modules\n",
    "%%capture\n",
    "!pip install docx2txt\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25e1de",
   "metadata": {
    "id": "1e25e1de"
   },
   "outputs": [],
   "source": [
    "#import the document loader for word documents\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "#for readability I use for loop\n",
    "#alternativly list comprehension saves two more lines\n",
    "#docs = [doc for path in doc_paths for doc in Docx2txtLoader(path).load()]\n",
    "docs = []\n",
    "for path in doc_paths:\n",
    "    docs.extend(Docx2txtLoader(path).load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e84a5d",
   "metadata": {
    "id": "c8e84a5d"
   },
   "source": [
    "## 2. Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e6fb2",
   "metadata": {
    "id": "0f6e6fb2"
   },
   "outputs": [],
   "source": [
    "#import a text splitter that converts the documents into manageable size\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500, #the total size of a chunck is limited to 1500 characters\n",
    "    chunk_overlap = 150 #the following chunck overlaps the previous chucks last 150 characters\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50dad10",
   "metadata": {
    "id": "d50dad10"
   },
   "source": [
    "## 3. Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U41gW-Z29hT-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U41gW-Z29hT-",
    "outputId": "139c6253-1777-40cc-b9e8-08af46262470"
   },
   "outputs": [],
   "source": [
    "#install the necessary modules\n",
    "%%capture\n",
    "!pip install openai\n",
    "!pip install chromadb\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s0WgBjmT9z6l",
   "metadata": {
    "id": "s0WgBjmT9z6l"
   },
   "source": [
    "You have to create an account with [OpenAI](https://openai.com/product) and create an API key to use their models within Langchain.\n",
    "\n",
    "to access the API key you can follow these steps:\n",
    "1. Log in to your account\n",
    "2. Select API\n",
    "3. Click on \"Personal\" (top right corner)\n",
    "4. Click on \"Manage API keys\"\n",
    "5. Click on \"Create new secret key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZ2h4eYB82WY",
   "metadata": {
    "id": "tZ2h4eYB82WY"
   },
   "outputs": [],
   "source": [
    "#better option is to save your API key as system variable\n",
    "openai_api_key = \"provide your OpenAI API key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0cd53",
   "metadata": {
    "id": "cbe0cd53"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#create a vector database that stores the documents and their respective embeddings\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "    persist_directory='./chroma/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4eed0",
   "metadata": {
    "id": "74e4eed0"
   },
   "source": [
    "## 4. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd7772",
   "metadata": {
    "id": "7ecd7772"
   },
   "outputs": [],
   "source": [
    "#set vector database as retriever for the RetrievalQA chain\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d40fd",
   "metadata": {
    "id": "9a2d40fd"
   },
   "source": [
    "## 5. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179912b",
   "metadata": {
    "id": "8179912b"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "#create a RetrievalQA chain \n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(openai_api_key=openai_api_key),\n",
    "                                      chain_type=\"stuff\",\n",
    "                                      retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"Ask any question about your documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ed480",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "The process above works just fine and is totally fine in case you only need these documents once. \n",
    "If you want to come back laters and ask questions about information contained in your documents, you do not need to run everything above. This would be very expensive in the long run because embedding your documents during the creation of the vector database is not free of charge. The code actually creates a directory where all the documents and their embeddings are saved. So you can step the first steps and just load the saved embeddings. You can use the following code instead right before step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "#load the vector database from saved directory\n",
    "db = Chroma(persist_directory=\"./chroma\", embedding_function = OpenAIEmbeddings())\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a73e1f",
   "metadata": {},
   "source": [
    "Step 5 is exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "#create a RetrievalQA chain \n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(openai_api_key=openai_api_key),\n",
    "                                      chain_type=\"stuff\",\n",
    "                                      retriever=retriever)\n",
    "\n",
    "qa.run(\"Ask any question about your documents\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
